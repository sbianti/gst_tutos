		GStreamer Basics
		¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯

Basé sur les tutos de gstreamer (http://docs.gstreamer.com) VERSION 0.10
http://www.freedesktop.org/software/gstreamer-sdk/data/docs/2012.5/gstreamer-0.10/libgstreamer.html
/!\ Implémentation en VERSION 1.0


I) hello.c:
¯¯¯¯¯¯¯¯¯¯
http://docs.gstreamer.com/pages/viewpage.action?pageId=327735

Un pipeline:
	Ensemble d'objets ordonnés-chaînés qui débute par des sources (producteurs de données) et se termine par des “sinks” (consommateurs des données).
	Entre ces 2 extrémités, on peut avoir des filtres, des démuxers…

A/
gst_init (&argc, &argv);
	→ trivial

B/
GstElement *pipeline = gst_parse_launch("playbin[2] uri=[proto]://[path]", NULL);
	→ Crée un pipeline tout-en-un de type playbin (resp. playbin2 en 0.10)

C/
gst_element_set_state(pipeline, GST_STATE_PLAYING);
return {GST_STATE_CHANGE_FAILURE, GST_STATE_CHANGE_SUCCESS, GST_STATE_CHANGE_ASYNC, GST_STATE_CHANGE_NO_PREROLL}
	→ Change l'état du pipeline qui est exécuté dans un autre thread.
GST_STATE_VOID_PENDING → GST_STATE_NULL → GST_STATE_READY → GST_STATE_PAUSED → GST_STATE_PLAYING

D/
GstBus *bus = gst_element_get_bus(pipeline);
	→ Récupère le bus de messages générés par le pipeline (car autre thread)

E/
GstMessage *msg = gst_bus_timed_pop_filtered(bus, GST_CLOCK_TIME_NONE, GST_MESSAGE_ERROR | GST_MESSAGE_EOS);
	→ Récupération synchrone des messages: bloque à l'infini (GST_CLOCK_TIME_NONE) tant qu'on ne reçoit ni erreur, ni EOS.


II) concept.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯
http://docs.gstreamer.com/display/GstSDK/Basic+tutorial+2%3A+GStreamer+concepts

 Qu'est-ce qu'un GstElement?
 Comment connecter les éléments entre eux ?
 Comment adapter le comportement d'un élément ?
 Comment observer le bus d'évènement du pipeline et en extraire l'infos des messages ?

A/
GstElement *source = gst_element_factory_make("videotestsrc", "la source");
GstElement *sink = gst_element_factory_make("autovideosink", "le sink");
	→ On crée 2 objets pour constituer notre pipeline.
	  Le nom donné en 2ième paramètre permet de retrouver cet élément si on a perdu le pointeur et le 1er param. est le type d'objet:
	  "videotestsrc" est une source video.
	  "autovideosink" est un «sink» qui choisi la méthode la mieux adaptée à notre système → indépendance de la plateforme.

B/
GstElement *pipeline = gst_pipeline_new("test-pipeline");

C/
gst_bin_add_many(GST_BIN (pipeline), sink, source, NULL);
	→ Un pipeline est un conteneur (GstBin) qui permet la synchro de tous les éléments qu'il contient. Il est donc essentiel. L'ordre n'a pas d'importance ici.

D/
gboolean gst_element_link(source, sink);
	→ Lie la source VERS le sink. Ils doivent être dans le même GstBin.
	  L'ordre est crucial ici.

E/
g_object_set(source, "pattern1", val1, "pattern2", val2, NULL);
	→ Les GstElement sont des GObject, on peut dont les manipuler comme ça.
	  Voir les propriétés de “videotestsrc”.
	  Rq: gst-inspect [élément-name] permet d'inspecter les propriétés de tous les éléments, ou d'un en particulier.

F/
gst_element_set_state ; gst_element_get_bus ; gst_bus_timed_pop_filtered

if (msg != NULL) {
   GError *err;
    gchar *debug_info;

    switch (GST_MESSAGE_TYPE (msg)) {
      case GST_MESSAGE_ERROR:
        gst_message_parse_error (msg, &err, &debug_info);
        fprintf(stderr, "Erreur reçu de l'élément «%s»: %s\n",
		GST_OBJECT_NAME (msg->src), err->message);
        fprintf(stderr, "Debugging info: %s\n",
		debug_info ? debug_info : "none");
        g_clear_error (&err);
        g_free(debug_info);
        break;
      case GST_MESSAGE_EOS:
        g_print("End-Of-Stream reached.\n");
        break;
      default:
        /* We should not reach here because we only asked for ERRORs and EOS */
        fprintf(stderr, "Unexpected message received.\n");
        break;
    }
    gst_message_unref(msg);
}

	→ Les GstMessage sont des structures assez complexes qui ne se manipulent qu'avec des macros comme GST_MESSAGE_TYPE(msg), GST_OBJECT_NAME(msg->src)


III) dynamic_pipelines.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Être notifié des signaux envoyés par les GstElement.
 Connecter les GstPad.

A/
data.source = gst_element_factory_make ("uridecodebin", "la source");
data.audio_convert = gst_element_factory_make("audioconvert", "le convertisseur");
data.audio_sink = gst_element_factory_make("autoaudiosink", "le sinkronizeur");
	→ “uridecodebin” est une source au sens large (une vrai source + démuxage + décodage des données). Elle fait la moitié du travail que fait playbin.
	  “audioconvert” est un convertisseur de format (portabilité).
	  “autoaudiosink” est équivalent à autovideosink pour l'audio.

B/
On ne lie que data.audio_convert à data.audio_sink car on ne sait pas encore comment lier data.source à data.audio_convert (on ne sait pas ce que contient la source)

C/
g_object_set (data.source, "uri", cmd, NULL);
	→ trivial

D/
g_signal_connect(data.source, "pad-added", G_CALLBACK(handler), &data);
	→ “uridecodebin” crée des GstPad à la volée en analysant le flux reçu. Il va falloir les lier à l'“audioconvert”.

E/ Callback pad_added_handler()
⋅Récupérer le sink pad du convert ET vérifier qu'il n'est pas déjà lié
⋅Récupérer les aptitudes du pad reçu (gst_pad_query_caps ⇒ gst_caps_get_structure ⇒ gst_structure_get_name (donne le mime-type)) pour vérifier ce qu'on reçoit et aviser du traitement à effectuer
⋅gst_pad_link(new_pad, sink_pad);
	→ Créer le lien souhaité, les 2 pads doivent être contenus dans le même bin.

** Voir I) et II) de HowTo-Playback.txt **


IV) basic-4_time_management.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Recueillir des infos du pipeline ⇒ cela requiert l'utilisation de GstQuery.
 Se déplacer dans le stream en live.

A/
msg = gst_bus_timed_pop_filtered(bus, 100 * GST_MSECOND,
			GST_MESSAGE_STATE_CHANGED | _ERROR | _EOS | _DURATION);
 ____________________________________________________________
|Utilisation des gst_query:                                  |
|	gst_element_query_position(pipeline, format, &pos);  |
|	gst_element_query_duration(pipeline, format, &durée);|
|
|	ou:
|	query = gst_query_new_seeking(TYPE)                  |
|	if (gst_element_query(pipeline, query))		     |
|	   gst_query_parse_seeking(query…)		     |
|	gst_query_unref(query);				     |
 ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
	→ On poll le bus en sortant tous les 100⋅10¯³ secondes
	  Si on est «PLAYING», on rafraichie notre UI.
	  rq: ici GstElement fournie de quoi récupérer les bonnes infos sans passer par des GstQuery (&pos et &durée de l'exemple).

B/
g_print ("Position %" GST_TIME_FORMAT " / %" GST_TIME_FORMAT "\r",
    GST_TIME_ARGS (current), GST_TIME_ARGS (data.duration));
	→ Petit affichage funky

C/
gst_element_seek_simple(data.playbin, GST_FORMAT_TIME,
			GST_SEEK_FLAG_FLUSH | GST_SEEK_FLAG_KEY_UNIT,
			30 * GST_SECOND);
	→ Un seek «simple»: TIME → not byte; FLUSH → vire toutes les données déjà dans le pipeline, KEY_UNIT → va à la trame clef la plus proche et consomme directement les données (sinon, progresse à l'[octet/tps] qui correspond au seek puis consomme les données);

D/ handle_message()
case GST_MESSAGE_DURATION_CHANGED:
	data->duration = GST_CLOCK_TIME_NONE;
	→ La durée d'un stream peut changer en cours de lecture: on peut soit la changer ici, soit la mettre invalide et la changer dans le polling du bus.

E/ handle_message()
if (data->playing) {
	GstQuery *query = gst_query_new_seeking(GST_FORMAT_TIME);
	if (gst_element_query(data->playbin, query))
		gst_query_parse_seeking(query, NULL, &seekable, &start, &end);
	→ Les requêtes de tout ordres marchent mieux en PAUSED voir en PLAYING.
	  On crée une requête de type {seek, TIME} et on la passe au pipeline.


V) basic-5_gui_toolkit.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Comment afficher la vidéo dans une fenêtre particulière ?
 Comment mettre à jour une UI à partir des infos de GStreamer.
 Comment ne recevoir que les messages qui nous intéressent.

 Généralement, les “sink” implémentent l'interface GstVideoOverlay (GstXOverlay en 0.10). C'est une «GObject interface» qui permet de spécifier la fenêtre où envoyer la vidéo au lieu d'en créer une exprès ; Il suffit par exemple de (pseudo-code):
	OVERLAY_CAST(mon_sink)->set_handle_window(…)
 et ça marche. Cependant, playbin, en tant que pipeline qui fait le café, fournie des fonctions qui vont bien (et ce, pour les nombreuses interfaces qu'il implémente).

 Dans cet exemple, on récupère la fenêtre créée par Gtk, et on l'applique à playbin.
 La vidéo s'affiche alors dans la Gtkwindow.

A/
#if defined (GDK_WINDOWING_X11)
#include <gdk/gdkx.h>
	→ Pour des raisons de portabilité, gdk fourni plusieurs entêtes différents pour gérer l'affichage.

B/
g_signal_connect(G_OBJECT(data.playbin), "[video|audio|text]-tags-changed",
		 (GCallback) tags_cb, &data);
	→ Être notifié des méta-données qui arrivent sur le stream avec :
void tags_cb(GstElement *ppl, gint stream, CustomData *data) qui fait
	gst_element_post_message(ppl,
		gst_message_new_application((GST_OBJECT(ppl),
			gst_structure_new("tags-changed", NULL))))
	→ Si on n'est pas dans le thread ppal, une modification des widgets Gtk est impossible, on doit donc informer le thread principal.
	 Ceci est une manière de faire différente de playback_1 où on récupère les méta-données lorsqu'on passe en STATE_PAUSED.

C/
gst_bus_add_signal_watch(bus);
g_signal_connect(G_OBJECT(bus), "message::<nom du message>",
		(GCallback)message_cb, &data);
	→ gst_bus_add_watch() permettait de récupérer tous les messages, ici, on sélectionne ceux qu'on veut traiter. Dans tout les cas, il faut une mainloop.
	 Elle est ici créée par gtk_main().

D/
gst_object_unref(bus);
	→ On en a plus besoin, ça n'est qu'une référence

E/
g_timeout_add_seconds(1, (GSourceFunc)refresh_ui, &data);
	→ Rafraîchir le slider chaque seconde.

F/ Realize callback
 Elle est appelée une fois, lors du premier affichage de la fenêtre.
GdkWindow *window = …
#if defined (GDK_WINDOWING_X11)
	guintptr window_handle = GDK_WINDOW_XID(window);
 Puis:
gst_[x|video]_overlay_set_window_handle(GST_[X|VIDEO]_OVERLAY(pipeline),
				      window_handle);
	→ API dépendante de la version de GStreamer.
	  On ne peut faire ça que lorsqu'on reçoit le «realize event».
	  Avant, la fenêtre n'est pas réalisée et gdk_window_ensure_native(win) échoue.

G/ Expose event
 Il ne faut le gérer que lorsqu'on est < STATE_PAUSED.
 Les “sink” gèrent le rafraichissement lorsqu'on est PLAYING ou PAUSED (on dit qu'il y a un flux de données dans les 2 cas).

E/ callback du slider ⇒ évident
	On pourrait ajouter une gestion plus fine du déplacement, genre tous les 0,3s max… dans cette implémentation, il se peut qu'un déplacement ne soit pas terminé lorsqu'un autre est demandé.

F/ callback refresh_ui & refresh_slider
 Faire avancer le slider si on est PLAYING /!!\ ou si on vient de passer en PAUSED
 Ici, on vérifie aussi si on a la durée du fichier.
	→ Ça pourrait être fait en passant en PAUSED la première fois ⇒ oui, mais un stream met souvent à jour cette info, il est donc mieux recueillir cette info à tout les coups.
	→ Est-ce que de nouvelles méta-données peuvent arriver dans un fichier sur disque ? ⇒ oui si c'est un dump de stream, sinon ?…
 On désactive “value-changed” au moment de bouger le curseur, sinon on va appeler slider_cb()

G/ tags_cb
 Elle est appelée lorsque des méta-données arrivent dans le flux. Elle est donc appelée dans un thread du pipeline, donc ce n'est pas le principal.
 On ne peut donc pas mettre à jour un composant de Gtk depuis cette fonction.
 Pour cela, on poste un message sur le bus du pipeline qui est écouté par le thread ppal.
gst_element_post_message(pipeline, GstMessage* m);
 avec m = gst_message_new_application(GST_OBJECT(pipeline),
				      gst_structure_new("tags-changed", NULL))
	→ cela crée un type de message réservé aux applis (Gstreamer n'en fera rien), accompagné de l'information name="tags-changed".


VI) basic-6_media-format_and_pad-caps.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Que sont les capacités d'un Pad ?
 Comment et quand les récupérer ?
 Pourquoi les connaître ?

Les pads permettent de faire entrer et sortir des infos des éléments.
Les capacités (caps) décrivent la nature des infos qui transitent par eux (vidéo RGB en 320×200 à 30 fps ; X264 en … à …)
Un pad peut supporter plusieurs capacités qui peuvent en plus avoir des ranges (audio de 1 à 48000 Hz).
Cependant, à un moment donné les infos qui transitent d'un pad à un autre doivent avoir un seul type.
Lorsqu'on lie 2 pads, une négociation aboutie au choix d'un type de capacités communes et à valeur fixe (pas de range), c'est pourquoi il faut connaître les pad-caps des éléments qu'on utilise (cela permet de comprendre pourquoi une négociation échoue).

⋅ Les schémas de connecteurs (pad templates):
  ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
Ce sont eux qui permettent d'instancier des connecteurs.
Exemple de sink pad templates pour un objet audio sink et une vidéo source:

*********************************************
SINK template: 'sink'
  Availability: Always
  Capabilities:
    audio/x-raw-int (⇒ x-raw en gstreamer-1.0)
               signed: true
                width: 16
                depth: 16
                 rate: [ 1, 2147483647 ]
             channels: [ 1, 2 ]
    audio/x-raw-int
               signed: false
                width: 8
                depth: 8
                 rate: [ 1, 2147483647 ]
             channels: [ 1, 2 ]
*********************************************
SRC template: 'src'
  Availability: Always
  Capabilities:
    video/x-raw-yuv
                width: [ 1, 2147483647 ]
               height: [ 1, 2147483647 ]
            framerate: [ 0/1, 2147483647/1 ]
               format: { I420, NV12, NV21, YV12, YUY2, Y42B, Y444, YUV9, YVU9, Y41B, Y800, Y8  , GREY, Y16 , UYVY, YVYU, IYU1, v308, AYUV, A420 }
*********************************************

Les […] sont des ranges et les {…} des listes.

N.B: gst-inspect permet d'avoir des infos sur les caps des éléments
     Certains éléments questionnent les couches basses pour sélectionner les caps à offrir. Donc, celles-ci seront différentes selon l'état de l'élément (READY, PAUSED…).

Ce tuto instantie 2 éléments à partir de leur “fabrique”, affiche leur caps-template, les lie et met le pipeline à PLAYING. À chaque changement d'état, les caps des sink pads sont affichées pour montrer le processus de négociation.

A/ print_pad_capabilities()
gst_element_get_static_pad(élément, pad_name)
	→ On récupère le pad «static» → ce sont les pad qui sont toujours disponibles.
gst_pad_get_current_caps(pad), gst_pad_query_caps(pad, NULL)
	→ On récupère le pad négocié, sinon, on se rabat sur toutes les capacités disponibles.

Utilisation des GstElementFactory, GstStaticPadTemplate.


VIb) basic-6b_media-format_and_pad-caps.c: Version 2
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
Permet de construire le pipeline sur la ligne de commande.
Utilisation de Tee (voir plus tard) ⇒ un peu (très ?) alambiqué. Vilain de chez vilain.
À effacer ?


VII) basic-7_multithreading_and_pad_availability.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 GStreamer crée un thread par flux, les greffons peuvent aussi en créer, mais comment en créer de nouveaux pour certaines parties du pipeline ?
 Qu'est-ce que la disponibilité des pads ?
 Comment copier des flux ?

⋅ Une partie de pipeline est appelée une branche et une application peut, en construisant un pipeline, préciser qu'une branche de ce pipeline peut s'exécuter dans un thread différent.
⋅ Il est parfois obligatoire de découper en plusieurs threads: lorsqu'il y a plusieurs sinks, chacun bloque son exécution tant que les autres ne sont pas prêts, si il n'y a qu'un thread, ceux-ci attendent le premier.
⋅ Il y a plusieurs types de pad:
     Les “always pads” → peuvent être liés dès l'état NULL avec gst_element_link.
     Les “sometimes pads” → sont créés lors de l'arrivée de données.
     Les “request pads” → créés à la demande (mais il peuvent quand même être liés avec gst_element_link, mais il faut alors penser à récupérer les pads créés automatiquement avec cette méthode pour les libérer après utilisation).

A/
Création des éléments, du pipeline…

B/
pad_template = gst_element_class_get_pad_template(
	     GST_ELEMENT_GET_CLASS(pipeline_component[TEE]), TEE_SRC_NAME);
	→ On récupère le schémat de connecteur du TEE.

t_pad_1 = gst_element_request_pad(pipeline_component[TEE],
				  tee_src_pad_template, NULL, NULL);
	→ On obtient un pad de nom "src_0"

q_1_pad = gst_element_get_static_pad(pipeline_component[AUDIO_QUEUE], "sink");
	→ Récupération classique d'un “always pad”

gst_pad_link(t_pad_1, q_1_pad);
	→ Liage de tout ça.

On répête ces opérations autant de fois que nécessaire pour créer autant de pad que l'on veut.

C/
gst_object_unref(q_1_pad);
	→ On peut faire ça dès que notre pipeline est créé.

gst_element_release_request_pad(pipeline_component[TEE], t_pad_1);
gst_object_unref(t_pad_1);
	→ A la fin seulement.


VIII) basic-8_short-cutting_pipeline.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Comment extraire et injecter des données dans le pipeline ?

Les éléments utilisés pour faire cela sont appsrc et appsink.
Ces deux éléments possèdent leur propre API ? Qui nécessitent de lier avec la lib gstreamer-app.
Cependant, il existe une approche plus simple, qui consiste à les contrôler avec des signaux.

L'élément appsrc peut fonctionner selon plusieurs modes différents.
	  pull-mode: il demande des données à l'appli chaque fois qu'il a besoin.
	  push-mode: l'appli envoie des données à son rythme. De plus, dans ce mode, l'appli peut être bloquée lorsque assez de données ont été fournies, ou bien écouter les signaux “enough-data” et “need-data” pour contrôler le débit.

Ce tuto utilise appsrc en pull-mode avec utilisation des signaux “enough-data” et “need-data”.

L'unité transmise entre pad est le GstBuffer.
Ces éléments possèdent un GstCaps, un horodatage et une durée.

A/
Création des éléments, link des always-pads, création des request-pads et link de ceux-ci.

B/
Connexion aux signaux "need-data" et "enough-data" de la source.
Configuration du sink avec "emit-signals" à TRUE, connexion à "new-sample"

Ça marche en 0.10, mais toujours pas en 1.0 ⇒ problème de négociation de pad.


IX) basic-9_media-info-gathering.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Comment extraire des infos d'une URI ?
 Comment savoir à l'avance si on peut jouer le contenu de cette URI ?

GstDiscoverer est l'élément à utiliser.
Il peut être utilisé de deux manières: synchrone ou asynchrone.

Ce tuto implémente une version simplifiée de l'outil gst-discoverer, sur le mode asynchrone..

A/
GstDiscoverer *discoverer = gst_discoverer_new(5*GST_SECOND, &err);
g_signal_connect(discoverer, "discovered", G_CALLBACK(callback), &data);
g_signal_connect(discoverer, "finished", G_CALLBACK(callback), &data);
	→ Création du GstDiscoverer, connexion des callbacks.

gst_discoverer_start(discoverer);
	→ démarrage d'une découverte asynchrone: si la liste des URI est vide, reste en attente.

gst_discoverer_discover_uri_async(discoverer, uri)
	→ ajoute une URI à la liste de discoverer. Si …start() à été appelé, le processus est actionné.

… = g_main_loop_new(NULL, FALSE);
  g_main_loop_run(loop);
  gst_discoverer_stop(discoverer);
	→ loop et stoppage du discovery avec vidage comme il faut de la liste.

B/ on_discovered_cb()
On récupère l'URI et le résultat.
Si c'est OK, divers affichages avec
   ⋅ gst_discoverer_info_get_*
   ⋅ itération sur la GstTagList récupérée par …_get_tags(info)
   ⋅ itération sur le GstDiscovererStreamInfo récupéré par …_get_stream_info(info) affichant les caractéristiques du stream ou du conteneur ainsi que les caps associées.


X) Gstreamer tools:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
Utilisation de gst-launch, gst-inspect, gst-…

A/ gst-launch-1.0:
   ¯¯¯¯¯¯¯¯¯¯¯¯¯¯
$ gst-launch playbin uri=file:///chemin (⇒ tuto #1)
$ gst-launch videotestsrc ! videoconvert ! autovideosink (tuto #3)

Modification d'une propriété d'un élément:
$ gst-launch videotestsrc pattern=4 ! videoconvert ! autovideosink (tuto #2)

Nommage d'un élément pour une réutilisation ultérieure:
$ gst-launch videotestsrc ! videoconvert ! tee name=t ! queue ! autovideosink t. ! queue ! autovideosink (tuto #7)
  → la notation <nom>. termine la branche et fait redescendre au niveau du Tee.

Sélection de pads spécifiques:
$ gst-launch souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! matroskademux name=d d.audio_0 ! vorbisparse ! matroskamux ! filesink location=sintel_video.mka
  → ne récupérer que l'audio de ce flux.

Sélectionner les pads correspondants à certaines capacités (caps filter):
$ gst-launch souphttpsrc location=http://docs.gstreamer.com/media/sintel_trailer-480p.webm ! matroskademux ! video/x-vp8 !  matroskamux ! filesink location=vidéo-seule.mkv

B/ gst-inspect:
   ¯¯¯¯¯¯¯¯¯¯¯
   ⇒ sans options, affiche tous les éléments disponibles. Sinon, affiche toutes les infos sur l'élément passé en paramètre.

C/ gst-discover:
   ¯¯¯¯¯¯¯¯¯¯¯¯
   ⇒ inspecter un média.


XI) Gstreamer debugging tool:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
La variable d'environnement GST_DEBUG est la clef:

0	none	Aucune info n'est affichée.
1	ERROR   Affiche toutes les erreurs fatales = les erreurs qui empèchent d'effectuer les actions demandées. L'application peut toujours s'en sortir si les conditions à l'origine de l'erreur sont gérées.
2	WARNING Affiche tous les warnings = les erreurs non-fatales. Des problèmes visibles pour l'utilisateur sont susceptibles de survenir.
3	INFO    Affiche tous les messages informatifs. Typiquement, ce sont les évènements qui arrivent une fois ou sont assez rares ou important pour être affichés à ce niveau.
4	DEBUG   Affiche tous les messages de débug. Ils sont relatifs aux évènements qui arrivent un nombre limité de fois pendant la vie de l'objet ; cela inclue la mise en route, la destruction, le changement de paramètres, ...
5	LOG     Affiche tous les messages. Ce sont les messages des évènements qui arrivent de manière répétée pendant la vie d'un objet ; la lecture classique et les évènements en état stable entre dans cette catégorie.

On peut aussi donner une valeur spécifique pour certains pluggins:
   $ GST_DEBUG=2,audiotestsrc:5 ./prog  ⇒ niveau 5 pour audiotestsrc et 2 pour le reste.
   $ GST_DEBUG=2,audio*:5 ./prog	⇒ 5 pour les pluggins dont le nom commence par audio, 2 pour les autres.

   $ gst-launch --gst-debug-help	⇒ donne la liste de toutes les catégories. Cette liste change à chaque fois qu'on installe/désinstalle un pluggin.

Détail des infos de débuggage:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
<date> <pid> <thread_id> <niveau de débug du message> <catégorie (plugin) de débug> <fichier et ligne>:<fonction>:<objet émetteur du message> <message>

Ajouter son propre débuggage:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
Cinq macros : GST_ERROR(), GST_WARNING(), GST_INFO(), GST_LOG() et GST_DEBUG()
Elles fonctionnent sur le mode de printf et utilise la catégorie “default”.
Pour la changer, il faut ajouter:
     GST_DEBUG_CATEGORY_STATIC (my_category);
     #define GST_CAT_DEFAULT my_category
Puis après gst_init():
     GST_DEBUG_CATEGORY_INIT (categorie, "ma categorie", 0, "bla bla bla");
⇒ Voir la doc de GST_DEBUG_CATEGORY_INIT.

Créer des graph de pipeline:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
$ GST_DEBUG_DUMP_DOT_DIR=. ./prog

Active la création de graph et les écrira dans le répertoire donné.

Pour cela, il faut utiliser les macros GST_DEBUG_BIN_TO_DOT_FILE() et GST_DEBUG_BIN_TO_DOT_FILE_WITH_TS().
gst-launch crée un graph à chaque changement d'état.

Pour créer une image avec un graph “.dot”:
dot -Tpng fichier.dot -o graph.png


XII) basic-12_streaming.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Infos additionnelles sur le streaming: Buffering et reprises d'interruptions (perte d'horloge).

Buffer:
L'élément Queue possède une liste de buffers. Dans ce tuto, on l'utilisera par l'intermédiaire de playbin.
Une application voulant gérer la mise en tampon peut écouter les messages venant de la Queue et mettre en pause la lecture lorsque son niveau est trop faible pour ne pas recevoir un EOS non sollicité.

Horloge:
Elle permet la synchronisation entre les différents sinks.
De nombreux éléments en fournissent une et GStreamer en sélectionne une parmis tous ces éléments.
Parfois, cette horloge est «perdue», il faut alors en sélectionner une autre.
Cela est fait en passant en PAUSED, puis en PLAYING.

A/
gst_element_set_state(pipeline, GST_STATE_PLAYING);
… if (ret == GST_STATE_CHANGE_NO_PREROLL) {
    data.is_live = TRUE;
  }

La valeur …NO_PREROLL est retournée par le passage en GST_STATE_PAUSED.
Cela signifie que le stream est live et que le passage en PAUSED est équivalent à PLAYING (un stream live ne peut pas être mis en pause).

Dans ce tuto, la mise en tampon sera désactivée si le stream est live.

B/
switch (GST_MESSAGE_TYPE(msg)) {
…
 case GST_MESSAGE_BUFFERING:
 gst_message_parse_buffering(msg, &percent);
 if (percent < 100)
    gst_element_set_state(data->pipeline, GST_STATE_PAUSED);
 else
    gst_element_set_state(data->pipeline, GST_STATE_PLAYING);

    → On exige un buffer rempli à 100% pour lire le flux.

C/
case GST_MESSAGE_CLOCK_LOST:
     gst_element_set_state(data->pipeline, GST_STATE_PAUSED);
     gst_element_set_state(data->pipeline, GST_STATE_PLAYING);

     → Triviale re-création d'horloge.

** Voir Playback-4 **


XIII) basic-13_playback-speed.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Utilisation des «trick modes» (contrôle de la vitesse et du sens de lecture, faire de l'image par image).

Il n'existe pas dans GStreamer de fonction pour changer uniquement la vitesse de lecture.
Cette valeur n'est qu'un paramètre parmis d'autres d'autres fonctions.

Deux évènements permettent de changer la vitesse : seek_event et step_event.

seek_event permet de se déplacer dans le fichier tout en changeant la vitesse (en positif ou négatif) alors que step_event permet d'avancer d'un certain nombre d'images à la fois tout en changeant la vitesse mais seulement vers l'avant.

Ces évènements sont envoyés dans le pipeline, mais si un bin reçoit un évènement, il le transmet à tous ses enfants ce qui n'est pas nécessaire : il suffit seulement d'envoyer l'évènement à un sink (vidéo ou audio, peu importe) et le reste se fait tout seul.

A/ Création du pipeline, installation de l'observateur d'entrées/sorties (g_io_add_watch).
	→ classique

B/ Création de l'évènement seek, envoie au pipeline.
  seek_event = gst_event_new_seek(data->rate, GST_FORMAT_TIME,
			 	  GST_SEEK_FLAG_FLUSH | GST_SEEK_FLAG_ACCURATE,
			 	  GST_SEEK_TYPE_NONE, -1, GST_SEEK_TYPE_NONE, -1);
	→ Dans le tuto original, on récupère la position en seconde pour faire un seek depuis cette position (pour une avance rapide) ou vers cette position (lecture inversée).
	  Cette manière de faire ne marche pas, inutile apparemment de différencier les deux cas, il suffit de mettre la vitesse que l'on veut et de ne pas se préocuper des positions de début et fin de segment avec GST_SEEK_TYPE_NONE à la place de GST_SEEK_TYPE_SET.

  g_object_get(data->pipeline, "video-sink", &data->video_sink, NULL);
  gst_element_send_event(data->video_sink, seek_event);
	→ trivial

C/ Évènement step
  data->playing = FALSE;
  gst_element_set_state(data->pipeline, GST_STATE_PAUSED);
  gst_element_send_event(data->video_sink,
			 gst_event_new_step(GST_FORMAT_BUFFERS, 1, rate, TRUE, FALSE));

	→ marche mieux en pause

/!\ Malgrès mes modifs, la lecture arrière ne marche qu'avec certains formats de fichiers, et encore, assez moyennement (je n'ai pas encore pu changer la vitesse correctement pendant une lecture arrière).
    Lecture arrière réussie avec un fichier flv. Lors du changement de la vitesse en arrière, j'obtiens des erreurs ”GStreamer-CRITICAL **: gst_segment_clip: assertion `segment->format == format' failed”


XIV) GStreamer handy elements:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Résumer des principaux éléments.

Les Bin:
¯¯¯¯¯¯¯
1) playbin      ⇒ Largement utilisé dans les tutos.
2) uridecodebin ⇒ Choisi une source pour décoder une URI en un flux brut et la connecte à decodebin.
	ex: gst-launch uridecodebin uri=… ! [audioconvert !] autovideosink
3) decodebin	⇒ contruit une branche contenant les décodeurs et multiplexeurs disponibles. Il en résulte éventuellement plusieurs source pads (autant que de flux contenus)
	ex: gst-launch souphttpsrc location=… ! decodebin ! autovideosink

Les file I/O:
¯¯¯¯¯¯¯¯¯¯¯¯
filesrc, filesink ⇒ vus
multisink ⇒ sauve chaque gstbuffer dans son propre fichier au lieu de tout mettre dans un seul fichier (utile avec des éléments comme ffenc_mjpeg)

Le réseau: souphttpsrc
¯¯¯¯¯¯¯¯¯
Les générateurs de flux: videotestsrc et audiotestsrc
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
Les adaptateurs vidéo:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
1) videoconvert [ffmpegcolorspace en 0.10] ⇒ convertit les espaces de couleur.
2) videorate ⇒ permet de faire correspondre le nombre d'images / seconde entre 2 éléments contiguës dans le pipeline. Il ajoute, enlève des images tant qu'il faut. Aucune interpolation n'est effectuée. Débute par une négociation entre les 2 éléments adjacents pour minimiser l'impacte sur les performances (donc il peut ne rien faire).
	ex: gst-launch-1.0 videotestsrc horizontal-speed=23 pattern=11 ! video/x-raw,framerate=30/1 ! videorate ! video/x-raw,framerate=1/1 ! autovideosink
3) videoscale ⇒ met à l'échelle la définition de la vidéo. Comme tous les adaptateurs, il débute son travail par une négociation pour tenter de ne rien faire.
	ex: gst-launch-1.0 uridecodebin uri=… ! videoscale ! video/x-raw,width=640,height=480 ! autovideosink

Les adaptateurs audio:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
1) audioconvert  ⇒ convertit des tampons audio brut en une multitude de formats. Conversion int → float ; quantification ; signe et endianness ; transformation de canaux. À toujours utiliser : … ! audioconvert ! autoaudiosink
2) audioresample ⇒ rééchantillonne l'audio.
	ex: gst-launch-1.0 uridecodebin uri=… ! audioresample ! audio/x-raw,rate=4000 ! audioconvert ! autoaudiosink
3) audiorate	 ⇒ à partir d'un flux brut, il produit un flux parfait en ajoutant ou enlevant des échantillons → il ne permet pas (à la différence de videorate) de changer la fréquence d'échantillonnage ! C'est un nettoyeur de flux, utile lorsqu'on récupère un flux où des échantillons ont étés perdus (par ex. dans un conteneur qui ne garde pas l'horodatage de l'audio).

Multithreading:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯
Queue      ⇒ elles stockent les données jusqu'à ce que éventuellement une certaine limite soit atteinte.
	     elles créent un thread pour découpler le travail entre le sink pad et le source pad.
multiqueue ⇒ crée des queues pour plusieurs flux simultanés, et en gère automatiquement l'utilisation comme la mise en tampon, la gestion des queues non-utilisées… C'est un élément qu'on utilise généralement pas directement (est utilisé dans decodebin).
tee	   ⇒ pour dupliquer un flux vers plusieurs sink pads.

Les capacités:
¯¯¯¯¯¯¯¯¯¯¯¯¯
capsfilter ⇒ sélection de capacités (gst-launch … ! video/x-raw-gray ! …) avec les GstCapsFilter.
	     Autre syntaxe: gst-launch … ! capsfilter caps="…" name=… ! …
	     Cette dernière permettant de donner un nom au capsfilter.
typefind   ⇒ détecte le type de média contenue dans le flux entrant et adapte son source-pad-caps à celui-ci avant d'émettre le signal “have-type”.
	     Il est utilisé par decodebin. On peut aussi utiliser les GstDiscoverer qui fournissent plus d'infos.

Le débugage:
¯¯¯¯¯¯¯¯¯¯¯
fakesink ⇒ sert à avaler les données en bout de pipeline pour tester si le reste marche bien. Il peut être très verbeux si on utilise -v avec gst-launch, il y a donc une propriétés pour le rendre silencieux:
	   gst-launch -v audiotestsrc num-buffers=1000 ! fakesink sync=false silent=true
identity ⇒ faux élément qui laisse passer les données sans les modifier. Il possède de nombreuses fonctionnalités de diagnostique
	   ex: gst-launch-1.0 audiotestsrc ! identity drop-probability=0.1 ! audioconvert ! autoaudiosink


XV) basic-15_clutter.c:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 Clutter est une bibliothèque graphique utilisant OpenGL pour le rendu.
 L'élément cluttersink de clutter-gst permet d'utiliser le flux issue d'un pipeline comme une texture.

[ rq: il faut clutter-gst-devel ; si intallé au niveau système, il ne fait pas le lien avec le gstreamer installé en local ]

A/ Initialisation
#include <clutter-gst/clutter-gst.h>
if (clutter_gst_init(&argc, &argv) != CLUTTER_INIT_SUCCESS) …
	→ encapsule le gst_init classique.

B/ Création d'une texture pour y afficher nos images
texture = CLUTTER_ACTOR(g_object_new(CLUTTER_TYPE_TEXTURE, "disable-slicing", TRUE, NULL));

C/ Création du sink: autocluttersink ou cluttersink si pas disponible

D/ Application de la texture au sink, et du sink au pipeline
g_object_set(sink, "texture", texture, NULL);
g_object_set(pipeline, "video-sink", sink, NULL);

E/ Bidouillage avec une timeline, des stages et des groupes de stages…

F/ Bidouillage avec les dimensions de la vidéo…
g_signal_connect(texture, "size-change", G_CALLBACK(size_change), NULL);


XVI) Plateform-specific elements:
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 «Tuto» dédié aux cas où on n'utilise pas les éléments qui font le café comme playbin, autovideosink…

Spécifiques à linux: ximagesink, xvimagesink, alsasink, pulsesink
Windows / Linux / MacOS: cluttersink

Rien de bien intéressant dans ce dernier tuto.
